{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3771624-590c-4c56-a97f-f86a14c477d1",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart?lang=python\n",
    "- https://ai.google.dev/gemini-api/docs/sdks\n",
    "- https://github.com/googleapis/python-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f62aaa-d958-4676-980d-bc192c494c5a",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- `pip install google-genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7fec7c-f979-437b-a5d4-1e1761fc8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a787d-d5a0-49a7-aedc-2c1b609377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./credentials.json\", \"r\") as json_file:\n",
    "    credentials = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7e056a-67ec-4392-816c-b71f082660ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = credentials[\"GoogleAIStudio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f5cfc1-04e5-4253-af81-6a33d012a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=credentials[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0402b90a-87dc-4b2a-8df6-00d9bd17258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemini-2.0-flash\",\n",
    "          \"gemini-2.0-flash-lite\",\n",
    "          \"gemini-2.0-flash-001\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f398e828-efc5-4b35-b30b-4103fc4e9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(model=\"gemini-2.0-flash\",\n",
    "                                          contents=\"How does RLHF work?\"\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e625a5d3-40cf-4489-a97a-e817f89f1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_from_response',\n",
       " '_get_text',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'automatic_function_calling_history',\n",
       " 'candidates',\n",
       " 'code_execution_result',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_time',\n",
       " 'dict',\n",
       " 'executable_code',\n",
       " 'from_orm',\n",
       " 'function_calls',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'model_version',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'parsed',\n",
       " 'prompt_feedback',\n",
       " 'response_id',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'text',\n",
       " 'to_json_dict',\n",
       " 'update_forward_refs',\n",
       " 'usage_metadata',\n",
       " 'validate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0835c0-aa7b-4e4a-ac7b-2076f1fa8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FieldInfo(annotation=Union[list[Candidate], NoneType], required=False, default=None, alias='candidates', alias_priority=1, description='Response variations returned by the model.\\n      ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_fields[\"candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b76669-41f7-46ec-9905-c57b843aed4a",
   "metadata": {},
   "source": [
    "## Managing Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08837b45-620b-45f9-915d-89661255b351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1393, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1393)], prompt_token_count=6, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=6)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=1399)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8075947-bfd5-4199-824e-8c02d8649be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1791"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.candidates_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2afdbf3-8616-40e8-8b44-eed1d2bac71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.prompt_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1fa0c49-21a5-4e8b-a6fd-2b546daa7e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.total_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da461c06-7b03-4e11-a660-3f0e776343a4",
   "metadata": {},
   "source": [
    "## Response Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c89bde-3fad-4901-ad2a-00f2a540d627",
   "metadata": {},
   "source": [
    "### Get number of response candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1acc1136-eb8a-44cd-9a96-08b43d31ba4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ceb5c7-2601-4e0d-8729-d4d4417a2218",
   "metadata": {},
   "source": [
    "### Candidate Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "647191f6-d4b5-4ff8-8ea2-4571aff5d9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3898322245450342"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].avg_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "474c73c1-8db1-4f5f-990a-7702062ac34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.candidates[0].token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79d16c48-4dc4-4b11-98a4-18e0b85b26b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.validate of <class 'google.genai.types.Candidate'>>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f9fd0da4-51a4-44e3-a424-97be359c521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_from_response',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'avg_logprobs',\n",
       " 'citation_metadata',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'finish_message',\n",
       " 'finish_reason',\n",
       " 'from_orm',\n",
       " 'grounding_metadata',\n",
       " 'index',\n",
       " 'json',\n",
       " 'logprobs_result',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'safety_ratings',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json_dict',\n",
       " 'token_count',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response.candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c34bebf-011d-4001-a4d3-f5115a88cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1de7e6-cb41-4e7a-b228-badb04d1c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF, or Reinforcement Learning from Human Feedback, is a technique used to fine-tune large language models (LLMs) to better align with human preferences and instructions. In simpler terms, it's a way to teach AI to be more helpful, harmless, and honest. Here's a breakdown of the process:\n",
      "\n",
      "**1. Supervised Fine-Tuning (SFT): Laying the Foundation**\n",
      "\n",
      "*   **Goal:**  Get the LLM to follow basic instructions and generate reasonably good text.\n",
      "*   **Process:**\n",
      "    *   Start with a pre-trained LLM (e.g., GPT-3, LLaMA).\n",
      "    *   Fine-tune it on a dataset of demonstrations. This dataset consists of prompts and ideal responses written by human annotators. Think of it like providing the model with examples of how to answer various questions and complete different tasks.\n",
      "    *   The model learns to mimic the style and content of the human-provided examples.\n",
      "*   **Outcome:** The resulting SFT model can generate text that is generally coherent and relevant but may still suffer from issues like:\n",
      "    *   Hallucinations (making things up).\n",
      "    *   Generating harmful or biased content.\n",
      "    *   Not perfectly adhering to nuanced instructions.\n",
      "\n",
      "**2. Reward Modeling (RM):  Quantifying Human Preferences**\n",
      "\n",
      "*   **Goal:** Train a reward model to predict how much a human would prefer one response over another. This is the core of learning from human feedback.\n",
      "*   **Process:**\n",
      "    *   **Data Collection:**  Generate multiple (typically 4-9) different responses from the SFT model for the *same* prompt.\n",
      "    *   **Human Ranking:** Present these responses to human annotators and ask them to rank them from best to worst based on specific criteria (e.g., helpfulness, accuracy, safety).  Crucially, this is *ranking*, not simply labeling responses as \"good\" or \"bad\". Ranking provides more granular information about preferences.\n",
      "    *   **Reward Model Training:** Train a separate model (the \"reward model\") to predict these rankings. This model takes a prompt and a response as input and outputs a scalar reward score.  The goal is for the reward model to assign higher scores to responses that humans preferred.  Common architectures for reward models include transformers, similar to the LLM being fine-tuned. The reward model is trained using a loss function that penalizes it for predicting incorrect preference rankings.  For example, if a human ranked response A higher than response B, the loss function encourages the reward model to give a higher score to A than to B.\n",
      "*   **Outcome:**  A reward model that can approximate human preferences, allowing the LLM to be optimized automatically.\n",
      "\n",
      "**3. Reinforcement Learning (RL): Optimizing the LLM**\n",
      "\n",
      "*   **Goal:** Fine-tune the LLM to maximize the reward signal provided by the reward model.\n",
      "*   **Process:**\n",
      "    *   **RL Algorithm:**  Use a reinforcement learning algorithm like Proximal Policy Optimization (PPO). PPO is a policy gradient method that tries to improve the LLM's policy (its response generation strategy) without drastically changing it in each iteration. This helps to ensure stable training.\n",
      "    *   **Interaction with the Environment:** The LLM (acting as an agent) generates responses to prompts (acting as the environment).\n",
      "    *   **Reward Calculation:**  The reward model evaluates the generated response and provides a reward score.\n",
      "    *   **Policy Update:** The RL algorithm uses this reward signal to update the LLM's parameters, encouraging it to generate responses that are more likely to receive high rewards from the reward model.\n",
      "    *   **Regularization:**  Important to prevent the LLM from \"gaming\" the reward model.  Often, a term is added to the reward function that penalizes the LLM for deviating too far from the original SFT model's behavior. This helps maintain the LLM's general knowledge and prevents it from generating overly specialized or nonsensical responses solely to maximize the reward. This regularization is crucial for the stability and generalization ability of RLHF.\n",
      "*   **Outcome:**  A fine-tuned LLM that generates responses that are better aligned with human preferences and instructions than the original SFT model.\n",
      "\n",
      "**In Summary (the TL;DR):**\n",
      "\n",
      "1.  **SFT:**  Get the LLM to follow instructions initially using supervised learning.\n",
      "2.  **RM:** Teach another model to *predict* human preferences using ranked responses.\n",
      "3.  **RL:** Use the predicted preferences (from the RM) to fine-tune the original LLM using reinforcement learning, rewarding it for generating preferred responses.\n",
      "\n",
      "**Why RLHF is important:**\n",
      "\n",
      "*   **Alignment:**  Aligns LLMs with human values and expectations, making them more helpful and less likely to generate harmful or biased content.\n",
      "*   **Instruction Following:**  Improves the LLM's ability to follow complex instructions and understand nuanced requests.\n",
      "*   **Generalization:**  Helps the LLM generalize to new tasks and domains.\n",
      "*   **Human-Centered AI:**  Promotes the development of AI systems that are designed to work in harmony with humans.\n",
      "\n",
      "**Challenges of RLHF:**\n",
      "\n",
      "*   **Data Collection:** Gathering high-quality human feedback is expensive and time-consuming.\n",
      "*   **Reward Model Bias:** The reward model can be biased by the preferences of the annotators, which can lead to the LLM generating responses that cater to those biases.  Careful selection and training of annotators are crucial.\n",
      "*   **Reward Hacking:**  The LLM may learn to exploit weaknesses in the reward model to generate responses that receive high rewards but are not actually helpful or aligned with human values.\n",
      "*   **Instability:** Reinforcement learning can be unstable, and the LLM may diverge from its desired behavior.\n",
      "*   **Scalability:** Scaling RLHF to very large LLMs and diverse tasks can be challenging.\n",
      "*   **Interpretability:** Understanding why the LLM generates certain responses after RLHF can be difficult.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "*   InstructGPT (from OpenAI)\n",
      "*   LaMDA (from Google)\n",
      "*   Anthropic's models\n",
      "\n",
      "RLHF is a complex and rapidly evolving field, but it has emerged as a powerful technique for aligning LLMs with human values and expectations. It's a key ingredient in building more helpful, harmless, and honest AI systems. As research continues, we can expect to see further improvements in the efficiency, stability, and scalability of RLHF, leading to even more capable and aligned LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915b145-1ba6-4a95-a2d7-f78a510f08c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
