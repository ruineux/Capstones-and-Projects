{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3771624-590c-4c56-a97f-f86a14c477d1",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "- \n",
    "- https://ai.google.dev/gemini-api/docs/quickstart?lang=python\n",
    "- https://ai.google.dev/gemini-api/docs/sdks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f62aaa-d958-4676-980d-bc192c494c5a",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7fec7c-f979-437b-a5d4-1e1761fc8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83a787d-d5a0-49a7-aedc-2c1b609377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\", \"r\") as json_file:\n",
    "    credentials = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff7e056a-67ec-4392-816c-b71f082660ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = credentials[\"GoogleAIStudio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f5cfc1-04e5-4253-af81-6a33d012a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=credentials[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402b90a-87dc-4b2a-8df6-00d9bd17258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemini-2.0-flash\",\n",
    "          \"gemini-2.0-flash-lite\",\n",
    "          \"gemini-2.0-flash-001\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f398e828-efc5-4b35-b30b-4103fc4e9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(model=\"gemini-2.0-flash\",\n",
    "                                          contents=\"How does RLHF work?\"\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e625a5d3-40cf-4489-a97a-e817f89f1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_from_response',\n",
       " '_get_text',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'automatic_function_calling_history',\n",
       " 'candidates',\n",
       " 'code_execution_result',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_time',\n",
       " 'dict',\n",
       " 'executable_code',\n",
       " 'from_orm',\n",
       " 'function_calls',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'model_version',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'parsed',\n",
       " 'prompt_feedback',\n",
       " 'response_id',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'text',\n",
       " 'to_json_dict',\n",
       " 'update_forward_refs',\n",
       " 'usage_metadata',\n",
       " 'validate']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb0835c0-aa7b-4e4a-ac7b-2076f1fa8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FieldInfo(annotation=Union[list[Candidate], NoneType], required=False, default=None, alias='candidates', alias_priority=1, description='Response variations returned by the model.\\n      ')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_fields[\"candidates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d91088-52a6-490b-b6e6-b24a68a8d5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08837b45-620b-45f9-915d-89661255b351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1561, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1561)], prompt_token_count=6, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=6)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=1567)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8075947-bfd5-4199-824e-8c02d8649be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.candidates_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2afdbf3-8616-40e8-8b44-eed1d2bac71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.prompt_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1fa0c49-21a5-4e8b-a6fd-2b546daa7e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.total_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1acc1136-eb8a-44cd-9a96-08b43d31ba4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='RLHF, or Reinforcement Learning from Human Feedback, is a powerful technique used to fine-tune large language models (LLMs) like GPT-3, LLaMA, or Bard, making them more aligned with human preferences and intentions. It essentially trains the model to behave in ways humans find helpful, harmless, and honest. Here\\'s a breakdown of how RLHF works, step-by-step:\\n\\n**1. Supervised Fine-Tuning (SFT):**\\n\\n*   **Purpose:** Get the LLM into a reasonable starting state.\\n*   **Process:**\\n    *   Gather a dataset of prompt-response pairs created by humans.  These pairs represent desirable behaviors and desired responses to specific prompts.\\n    *   Fine-tune the pre-trained LLM on this dataset using supervised learning. This teaches the model to generate outputs similar to those written by humans in the dataset.\\n    *   **Outcome:**  This creates an SFT model that can generate responses that are generally more coherent and relevant than the original pre-trained model. However, it might still produce outputs that are not perfectly aligned with human preferences or may exhibit undesirable behaviors.\\n\\n**2. Reward Model Training:**\\n\\n*   **Purpose:** Teach the model to *predict* which response is better according to human preferences. This is the key step for incorporating human feedback.\\n*   **Process:**\\n    *   **Data Collection (Preference Data):**\\n        *   Sample prompts from a diverse distribution.\\n        *   For each prompt, generate *multiple* responses using the SFT model (typically 4-9 different responses).  It\\'s good to have some variation in these responses.\\n        *   Have human labelers **rank** these responses from best to worst *based on specific criteria*. These criteria often include helpfulness, harmlessness, and truthfulness (the \"3 H\\'s\").  Often, labelers will directly compare pairs of responses and indicate which one is preferable.  This pairwise comparison approach is common because it\\'s easier and more reliable than absolute ranking.\\n    *   **Reward Model Training:**\\n        *   Train a separate model (the reward model) to predict the ranking or preference score given by the human labelers.  This reward model typically takes a prompt and a response as input and outputs a scalar value representing how desirable that response is according to human preferences.\\n        *   The reward model is often based on the same architecture as the LLM being fine-tuned (e.g., a smaller version of GPT).\\n        *   The loss function used to train the reward model is designed to maximize the difference in predicted reward between preferred and dispreferred responses.  For example, if response A is preferred over response B, the reward model should assign a higher score to A than to B.\\n    *   **Outcome:**  A reward model that can approximate human judgment of the quality of a response based on its helpfulness, harmlessness, and truthfulness.  This model acts as an automatic \"critic\" that can assess the desirability of responses generated by the LLM.\\n\\n**3. Reinforcement Learning Fine-Tuning:**\\n\\n*   **Purpose:** Optimize the SFT model to generate responses that maximize the reward model\\'s score, effectively aligning the LLM\\'s behavior with human preferences as captured by the reward model.\\n*   **Process:**\\n    *   Use the trained reward model as the reward function in a reinforcement learning algorithm. A common algorithm used is Proximal Policy Optimization (PPO).\\n    *   **Reinforcement Learning Loop:**\\n        *   **Prompt Generation:** The LLM (now called the *policy*) is given a prompt.\\n        *   **Response Generation:** The policy generates a response.\\n        *   **Reward Evaluation:** The reward model scores the generated response.\\n        *   **Policy Update:** The RL algorithm (e.g., PPO) updates the LLM\\'s parameters to increase the probability of generating responses that receive high scores from the reward model.\\n    *   **Regularization:** To prevent the LLM from deviating too far from the SFT model (which could lead to unintended consequences or a loss of general knowledge), a regularization term is often added to the reward function.  This term penalizes the LLM for generating responses that are too different from those it would have generated before RLHF.  Specifically, it minimizes the KL divergence between the RL fine-tuned model and the initial SFT model.\\n    *   **Outcome:** The final RLHF model is better aligned with human preferences than the original SFT model. It will tend to generate responses that are rated as more helpful, harmless, and truthful by human labelers.\\n\\n**In summary, RLHF consists of these core steps:**\\n\\n1.  **Supervised Fine-Tuning (SFT):**  Create a reasonable starting point for the LLM.\\n2.  **Reward Model Training:** Train a model to predict human preferences from ranked responses.\\n3.  **Reinforcement Learning Fine-Tuning:** Optimize the LLM to maximize the reward model\\'s score, aligning it with human preferences.\\n\\n**Key Advantages of RLHF:**\\n\\n*   **Alignment with Human Preferences:** Significantly improves the alignment of LLMs with human values and expectations.\\n*   **Reduced Harmful Outputs:**  Helps reduce the generation of toxic, biased, or misleading content.\\n*   **Improved Helpfulness:** Leads to more informative and useful responses.\\n*   **Better Truthfulness:**  Encourages the model to be more accurate and less likely to hallucinate information.\\n\\n**Challenges of RLHF:**\\n\\n*   **Data Collection Costs:** Gathering high-quality preference data from human labelers is expensive and time-consuming.\\n*   **Reward Model Bias:** The reward model can be biased by the preferences of the human labelers, which may not perfectly represent the preferences of all users.\\n*   **Optimization Challenges:** Training the RLHF model can be challenging and require careful tuning of hyperparameters.  It\\'s possible to over-optimize the reward, leading to unexpected and undesirable behavior (a phenomenon known as \"reward hacking\").\\n*   **Generalization:** Ensuring that the model generalizes well to unseen prompts and scenarios can be difficult.  The reward model may not be accurate for all possible inputs.\\n*   **Scalability:** Scaling RLHF to very large models and datasets can be computationally expensive.\\n*   **Interpretability:** It can be difficult to understand why the RLHF model makes the decisions it does.\\n\\n**Why not just use supervised learning on the ranked responses?**\\n\\nWhile supervised learning on ranked data is possible, it has limitations.  The key benefit of the *reward model* is that it provides a *continuous* signal (the reward score) representing the quality of a response. This continuous signal allows the RL algorithm to explore a wider range of possible behaviors and optimize the LLM\\'s policy more effectively.  Supervised learning on ranked data often only provides a discrete set of labels (e.g., \"best\", \"second best\", etc.), which can be less informative. The reward model distills the ranking information into a more nuanced and adaptable signal for reinforcement learning.\\n\\nRLHF is a complex but powerful technique that has become essential for building safe and reliable LLMs. Despite its challenges, it remains the dominant approach for aligning these models with human values and preferences. As research continues, we can expect to see further improvements in the efficiency, scalability, and robustness of RLHF.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.4484889869274504, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9fd0da4-51a4-44e3-a424-97be359c521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='RLHF, or Reinforcement Learning from Human Feedback, is a powerful technique used to fine-tune large language models (LLMs) like GPT-3, LLaMA, or Bard, making them more aligned with human preferences and intentions. It essentially trains the model to behave in ways humans find helpful, harmless, and honest. Here\\'s a breakdown of how RLHF works, step-by-step:\\n\\n**1. Supervised Fine-Tuning (SFT):**\\n\\n*   **Purpose:** Get the LLM into a reasonable starting state.\\n*   **Process:**\\n    *   Gather a dataset of prompt-response pairs created by humans.  These pairs represent desirable behaviors and desired responses to specific prompts.\\n    *   Fine-tune the pre-trained LLM on this dataset using supervised learning. This teaches the model to generate outputs similar to those written by humans in the dataset.\\n    *   **Outcome:**  This creates an SFT model that can generate responses that are generally more coherent and relevant than the original pre-trained model. However, it might still produce outputs that are not perfectly aligned with human preferences or may exhibit undesirable behaviors.\\n\\n**2. Reward Model Training:**\\n\\n*   **Purpose:** Teach the model to *predict* which response is better according to human preferences. This is the key step for incorporating human feedback.\\n*   **Process:**\\n    *   **Data Collection (Preference Data):**\\n        *   Sample prompts from a diverse distribution.\\n        *   For each prompt, generate *multiple* responses using the SFT model (typically 4-9 different responses).  It\\'s good to have some variation in these responses.\\n        *   Have human labelers **rank** these responses from best to worst *based on specific criteria*. These criteria often include helpfulness, harmlessness, and truthfulness (the \"3 H\\'s\").  Often, labelers will directly compare pairs of responses and indicate which one is preferable.  This pairwise comparison approach is common because it\\'s easier and more reliable than absolute ranking.\\n    *   **Reward Model Training:**\\n        *   Train a separate model (the reward model) to predict the ranking or preference score given by the human labelers.  This reward model typically takes a prompt and a response as input and outputs a scalar value representing how desirable that response is according to human preferences.\\n        *   The reward model is often based on the same architecture as the LLM being fine-tuned (e.g., a smaller version of GPT).\\n        *   The loss function used to train the reward model is designed to maximize the difference in predicted reward between preferred and dispreferred responses.  For example, if response A is preferred over response B, the reward model should assign a higher score to A than to B.\\n    *   **Outcome:**  A reward model that can approximate human judgment of the quality of a response based on its helpfulness, harmlessness, and truthfulness.  This model acts as an automatic \"critic\" that can assess the desirability of responses generated by the LLM.\\n\\n**3. Reinforcement Learning Fine-Tuning:**\\n\\n*   **Purpose:** Optimize the SFT model to generate responses that maximize the reward model\\'s score, effectively aligning the LLM\\'s behavior with human preferences as captured by the reward model.\\n*   **Process:**\\n    *   Use the trained reward model as the reward function in a reinforcement learning algorithm. A common algorithm used is Proximal Policy Optimization (PPO).\\n    *   **Reinforcement Learning Loop:**\\n        *   **Prompt Generation:** The LLM (now called the *policy*) is given a prompt.\\n        *   **Response Generation:** The policy generates a response.\\n        *   **Reward Evaluation:** The reward model scores the generated response.\\n        *   **Policy Update:** The RL algorithm (e.g., PPO) updates the LLM\\'s parameters to increase the probability of generating responses that receive high scores from the reward model.\\n    *   **Regularization:** To prevent the LLM from deviating too far from the SFT model (which could lead to unintended consequences or a loss of general knowledge), a regularization term is often added to the reward function.  This term penalizes the LLM for generating responses that are too different from those it would have generated before RLHF.  Specifically, it minimizes the KL divergence between the RL fine-tuned model and the initial SFT model.\\n    *   **Outcome:** The final RLHF model is better aligned with human preferences than the original SFT model. It will tend to generate responses that are rated as more helpful, harmless, and truthful by human labelers.\\n\\n**In summary, RLHF consists of these core steps:**\\n\\n1.  **Supervised Fine-Tuning (SFT):**  Create a reasonable starting point for the LLM.\\n2.  **Reward Model Training:** Train a model to predict human preferences from ranked responses.\\n3.  **Reinforcement Learning Fine-Tuning:** Optimize the LLM to maximize the reward model\\'s score, aligning it with human preferences.\\n\\n**Key Advantages of RLHF:**\\n\\n*   **Alignment with Human Preferences:** Significantly improves the alignment of LLMs with human values and expectations.\\n*   **Reduced Harmful Outputs:**  Helps reduce the generation of toxic, biased, or misleading content.\\n*   **Improved Helpfulness:** Leads to more informative and useful responses.\\n*   **Better Truthfulness:**  Encourages the model to be more accurate and less likely to hallucinate information.\\n\\n**Challenges of RLHF:**\\n\\n*   **Data Collection Costs:** Gathering high-quality preference data from human labelers is expensive and time-consuming.\\n*   **Reward Model Bias:** The reward model can be biased by the preferences of the human labelers, which may not perfectly represent the preferences of all users.\\n*   **Optimization Challenges:** Training the RLHF model can be challenging and require careful tuning of hyperparameters.  It\\'s possible to over-optimize the reward, leading to unexpected and undesirable behavior (a phenomenon known as \"reward hacking\").\\n*   **Generalization:** Ensuring that the model generalizes well to unseen prompts and scenarios can be difficult.  The reward model may not be accurate for all possible inputs.\\n*   **Scalability:** Scaling RLHF to very large models and datasets can be computationally expensive.\\n*   **Interpretability:** It can be difficult to understand why the RLHF model makes the decisions it does.\\n\\n**Why not just use supervised learning on the ranked responses?**\\n\\nWhile supervised learning on ranked data is possible, it has limitations.  The key benefit of the *reward model* is that it provides a *continuous* signal (the reward score) representing the quality of a response. This continuous signal allows the RL algorithm to explore a wider range of possible behaviors and optimize the LLM\\'s policy more effectively.  Supervised learning on ranked data often only provides a discrete set of labels (e.g., \"best\", \"second best\", etc.), which can be less informative. The reward model distills the ranking information into a more nuanced and adaptable signal for reinforcement learning.\\n\\nRLHF is a complex but powerful technique that has become essential for building safe and reliable LLMs. Despite its challenges, it remains the dominant approach for aligning these models with human values and preferences. As research continues, we can expect to see further improvements in the efficiency, scalability, and robustness of RLHF.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.4484889869274504, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5262ae17-9200-479b-b073-f15ddf8e7fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4484889869274504"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].avg_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c34bebf-011d-4001-a4d3-f5115a88cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1de7e6-cb41-4e7a-b228-badb04d1c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF, or Reinforcement Learning from Human Feedback, is a powerful technique used to fine-tune large language models (LLMs) like GPT-3, LLaMA, or Bard, making them more aligned with human preferences and intentions. It essentially trains the model to behave in ways humans find helpful, harmless, and honest. Here's a breakdown of how RLHF works, step-by-step:\n",
      "\n",
      "**1. Supervised Fine-Tuning (SFT):**\n",
      "\n",
      "*   **Purpose:** Get the LLM into a reasonable starting state.\n",
      "*   **Process:**\n",
      "    *   Gather a dataset of prompt-response pairs created by humans.  These pairs represent desirable behaviors and desired responses to specific prompts.\n",
      "    *   Fine-tune the pre-trained LLM on this dataset using supervised learning. This teaches the model to generate outputs similar to those written by humans in the dataset.\n",
      "    *   **Outcome:**  This creates an SFT model that can generate responses that are generally more coherent and relevant than the original pre-trained model. However, it might still produce outputs that are not perfectly aligned with human preferences or may exhibit undesirable behaviors.\n",
      "\n",
      "**2. Reward Model Training:**\n",
      "\n",
      "*   **Purpose:** Teach the model to *predict* which response is better according to human preferences. This is the key step for incorporating human feedback.\n",
      "*   **Process:**\n",
      "    *   **Data Collection (Preference Data):**\n",
      "        *   Sample prompts from a diverse distribution.\n",
      "        *   For each prompt, generate *multiple* responses using the SFT model (typically 4-9 different responses).  It's good to have some variation in these responses.\n",
      "        *   Have human labelers **rank** these responses from best to worst *based on specific criteria*. These criteria often include helpfulness, harmlessness, and truthfulness (the \"3 H's\").  Often, labelers will directly compare pairs of responses and indicate which one is preferable.  This pairwise comparison approach is common because it's easier and more reliable than absolute ranking.\n",
      "    *   **Reward Model Training:**\n",
      "        *   Train a separate model (the reward model) to predict the ranking or preference score given by the human labelers.  This reward model typically takes a prompt and a response as input and outputs a scalar value representing how desirable that response is according to human preferences.\n",
      "        *   The reward model is often based on the same architecture as the LLM being fine-tuned (e.g., a smaller version of GPT).\n",
      "        *   The loss function used to train the reward model is designed to maximize the difference in predicted reward between preferred and dispreferred responses.  For example, if response A is preferred over response B, the reward model should assign a higher score to A than to B.\n",
      "    *   **Outcome:**  A reward model that can approximate human judgment of the quality of a response based on its helpfulness, harmlessness, and truthfulness.  This model acts as an automatic \"critic\" that can assess the desirability of responses generated by the LLM.\n",
      "\n",
      "**3. Reinforcement Learning Fine-Tuning:**\n",
      "\n",
      "*   **Purpose:** Optimize the SFT model to generate responses that maximize the reward model's score, effectively aligning the LLM's behavior with human preferences as captured by the reward model.\n",
      "*   **Process:**\n",
      "    *   Use the trained reward model as the reward function in a reinforcement learning algorithm. A common algorithm used is Proximal Policy Optimization (PPO).\n",
      "    *   **Reinforcement Learning Loop:**\n",
      "        *   **Prompt Generation:** The LLM (now called the *policy*) is given a prompt.\n",
      "        *   **Response Generation:** The policy generates a response.\n",
      "        *   **Reward Evaluation:** The reward model scores the generated response.\n",
      "        *   **Policy Update:** The RL algorithm (e.g., PPO) updates the LLM's parameters to increase the probability of generating responses that receive high scores from the reward model.\n",
      "    *   **Regularization:** To prevent the LLM from deviating too far from the SFT model (which could lead to unintended consequences or a loss of general knowledge), a regularization term is often added to the reward function.  This term penalizes the LLM for generating responses that are too different from those it would have generated before RLHF.  Specifically, it minimizes the KL divergence between the RL fine-tuned model and the initial SFT model.\n",
      "    *   **Outcome:** The final RLHF model is better aligned with human preferences than the original SFT model. It will tend to generate responses that are rated as more helpful, harmless, and truthful by human labelers.\n",
      "\n",
      "**In summary, RLHF consists of these core steps:**\n",
      "\n",
      "1.  **Supervised Fine-Tuning (SFT):**  Create a reasonable starting point for the LLM.\n",
      "2.  **Reward Model Training:** Train a model to predict human preferences from ranked responses.\n",
      "3.  **Reinforcement Learning Fine-Tuning:** Optimize the LLM to maximize the reward model's score, aligning it with human preferences.\n",
      "\n",
      "**Key Advantages of RLHF:**\n",
      "\n",
      "*   **Alignment with Human Preferences:** Significantly improves the alignment of LLMs with human values and expectations.\n",
      "*   **Reduced Harmful Outputs:**  Helps reduce the generation of toxic, biased, or misleading content.\n",
      "*   **Improved Helpfulness:** Leads to more informative and useful responses.\n",
      "*   **Better Truthfulness:**  Encourages the model to be more accurate and less likely to hallucinate information.\n",
      "\n",
      "**Challenges of RLHF:**\n",
      "\n",
      "*   **Data Collection Costs:** Gathering high-quality preference data from human labelers is expensive and time-consuming.\n",
      "*   **Reward Model Bias:** The reward model can be biased by the preferences of the human labelers, which may not perfectly represent the preferences of all users.\n",
      "*   **Optimization Challenges:** Training the RLHF model can be challenging and require careful tuning of hyperparameters.  It's possible to over-optimize the reward, leading to unexpected and undesirable behavior (a phenomenon known as \"reward hacking\").\n",
      "*   **Generalization:** Ensuring that the model generalizes well to unseen prompts and scenarios can be difficult.  The reward model may not be accurate for all possible inputs.\n",
      "*   **Scalability:** Scaling RLHF to very large models and datasets can be computationally expensive.\n",
      "*   **Interpretability:** It can be difficult to understand why the RLHF model makes the decisions it does.\n",
      "\n",
      "**Why not just use supervised learning on the ranked responses?**\n",
      "\n",
      "While supervised learning on ranked data is possible, it has limitations.  The key benefit of the *reward model* is that it provides a *continuous* signal (the reward score) representing the quality of a response. This continuous signal allows the RL algorithm to explore a wider range of possible behaviors and optimize the LLM's policy more effectively.  Supervised learning on ranked data often only provides a discrete set of labels (e.g., \"best\", \"second best\", etc.), which can be less informative. The reward model distills the ranking information into a more nuanced and adaptable signal for reinforcement learning.\n",
      "\n",
      "RLHF is a complex but powerful technique that has become essential for building safe and reliable LLMs. Despite its challenges, it remains the dominant approach for aligning these models with human values and preferences. As research continues, we can expect to see further improvements in the efficiency, scalability, and robustness of RLHF.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
