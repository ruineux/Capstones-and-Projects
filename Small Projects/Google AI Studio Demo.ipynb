{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3771624-590c-4c56-a97f-f86a14c477d1",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart?lang=python\n",
    "- https://ai.google.dev/gemini-api/docs/sdks\n",
    "- https://github.com/googleapis/python-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f62aaa-d958-4676-980d-bc192c494c5a",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- `pip install google-genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7fec7c-f979-437b-a5d4-1e1761fc8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83a787d-d5a0-49a7-aedc-2c1b609377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./credentials\", \"r\") as json_file:\n",
    "    credentials = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff7e056a-67ec-4392-816c-b71f082660ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = credentials[\"GoogleAIStudio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f5cfc1-04e5-4253-af81-6a33d012a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=credentials[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402b90a-87dc-4b2a-8df6-00d9bd17258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemini-2.0-flash\",\n",
    "          \"gemini-2.0-flash-lite\",\n",
    "          \"gemini-2.0-flash-001\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f398e828-efc5-4b35-b30b-4103fc4e9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(model=\"gemini-2.0-flash\",\n",
    "                                          contents=\"How does RLHF work?\"\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e625a5d3-40cf-4489-a97a-e817f89f1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_from_response',\n",
       " '_get_text',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'automatic_function_calling_history',\n",
       " 'candidates',\n",
       " 'code_execution_result',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_time',\n",
       " 'dict',\n",
       " 'executable_code',\n",
       " 'from_orm',\n",
       " 'function_calls',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'model_version',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'parsed',\n",
       " 'prompt_feedback',\n",
       " 'response_id',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'text',\n",
       " 'to_json_dict',\n",
       " 'update_forward_refs',\n",
       " 'usage_metadata',\n",
       " 'validate']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb0835c0-aa7b-4e4a-ac7b-2076f1fa8d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FieldInfo(annotation=Union[list[Candidate], NoneType], required=False, default=None, alias='candidates', alias_priority=1, description='Response variations returned by the model.\\n      ')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_fields[\"candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b76669-41f7-46ec-9905-c57b843aed4a",
   "metadata": {},
   "source": [
    "## Managing Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08837b45-620b-45f9-915d-89661255b351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1791, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1791)], prompt_token_count=6, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=6)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=1797)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8075947-bfd5-4199-824e-8c02d8649be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1791"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.candidates_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2afdbf3-8616-40e8-8b44-eed1d2bac71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.prompt_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1fa0c49-21a5-4e8b-a6fd-2b546daa7e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata.total_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da461c06-7b03-4e11-a660-3f0e776343a4",
   "metadata": {},
   "source": [
    "## Response Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c89bde-3fad-4901-ad2a-00f2a540d627",
   "metadata": {},
   "source": [
    "### Get number of response candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1acc1136-eb8a-44cd-9a96-08b43d31ba4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ceb5c7-2601-4e0d-8729-d4d4417a2218",
   "metadata": {},
   "source": [
    "### Candidate Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "647191f6-d4b5-4ff8-8ea2-4571aff5d9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3898322245450342"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].avg_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "474c73c1-8db1-4f5f-990a-7702062ac34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.candidates[0].token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79d16c48-4dc4-4b11-98a4-18e0b85b26b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.validate of <class 'google.genai.types.Candidate'>>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f9fd0da4-51a4-44e3-a424-97be359c521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_from_response',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'avg_logprobs',\n",
       " 'citation_metadata',\n",
       " 'construct',\n",
       " 'content',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'finish_message',\n",
       " 'finish_reason',\n",
       " 'from_orm',\n",
       " 'grounding_metadata',\n",
       " 'index',\n",
       " 'json',\n",
       " 'logprobs_result',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'safety_ratings',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'to_json_dict',\n",
       " 'token_count',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response.candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c34bebf-011d-4001-a4d3-f5115a88cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc1de7e6-cb41-4e7a-b228-badb04d1c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF (Reinforcement Learning from Human Feedback) is a technique used to fine-tune large language models (LLMs) to better align with human preferences and instructions. In essence, it's a process of training a reward model based on human feedback, and then using that reward model to further train the LLM using reinforcement learning. Here's a breakdown of how it works, step-by-step:\n",
      "\n",
      "**1. Pre-training the LLM (Optional, but Usually Necessary):**\n",
      "\n",
      "*   **Purpose:**  Start with a pre-trained LLM.  This is crucial as it provides the model with a foundation of general language understanding and generation capabilities.\n",
      "*   **Method:** The LLM is typically pre-trained on a massive dataset of text and code using self-supervised learning.  Examples include training it to predict the next word in a sequence (causal language modeling) or to mask and predict missing words.  Examples of these base models include the GPT family from OpenAI, LLaMA from Meta, and others.\n",
      "\n",
      "**2. Generating Responses and Gathering Human Feedback:**\n",
      "\n",
      "*   **Prompting:**  The pre-trained LLM is presented with a variety of prompts or instructions.  These prompts are designed to elicit different kinds of responses from the model.\n",
      "*   **Generating Multiple Responses:** For each prompt, the LLM generates several different candidate responses.  The number of responses generated depends on the specific implementation, but it's usually a few (e.g., 4-9).\n",
      "*   **Human Evaluation:**  Human annotators (labelers, evaluators) are presented with the prompt and the multiple responses generated by the LLM.  Their task is to rank or rate the responses based on criteria like:\n",
      "    *   **Helpfulness:** How well does the response answer the prompt or solve the user's problem?\n",
      "    *   **Honesty:**  Is the response truthful and factually correct? Does it avoid making things up?\n",
      "    *   **Harmlessness:**  Is the response safe and ethical?  Does it avoid generating biased, discriminatory, or harmful content?\n",
      "    *   **Other Task-Specific Criteria:** Depending on the application, there might be other criteria specific to the task, like style, creativity, or adherence to specific formatting guidelines.\n",
      "*   **Preference Data Creation:**  The human evaluations are used to create a dataset of pairwise preferences.  For each prompt, the dataset indicates which response(s) are preferred over the others.  For example: \"For prompt X, response A is preferred over response B, and response B is preferred over response C.\"  Often, these preferences are expressed as a ranking rather than just a simple 'better than'. The strength of the preference is also sometimes captured.\n",
      "\n",
      "**3. Training the Reward Model:**\n",
      "\n",
      "*   **Purpose:**  The reward model is a separate model (often a transformer-based model, but smaller than the LLM) trained to predict the human preference for a given response to a prompt.  It learns to assign a *reward score* to each response based on how well it aligns with human preferences.\n",
      "*   **Method:**  The reward model is trained using the pairwise preference dataset created in the previous step. The model is trained to maximize the difference in reward score between the preferred response and the dispreferred response for each prompt.  The loss function often used is a Bradley-Terry model loss or a similar approach that models pairwise comparisons.\n",
      "*   **Input:** The input to the reward model is typically the prompt and the corresponding LLM-generated response.\n",
      "*   **Output:**  The output of the reward model is a scalar value (the reward score) representing how good the response is according to human preferences.\n",
      "\n",
      "**4. Fine-tuning the LLM with Reinforcement Learning:**\n",
      "\n",
      "*   **Purpose:** Use the trained reward model as a proxy for human feedback to further fine-tune the original LLM. This step aligns the LLM's behavior with the learned preferences.\n",
      "*   **Method:**  This is where the reinforcement learning (RL) part comes in.  The LLM is treated as an *agent* that interacts with an *environment* (defined by the prompts).\n",
      "    *   **State:** The state is the prompt itself.\n",
      "    *   **Action:** The action is the response generated by the LLM.\n",
      "    *   **Reward:** The reward is the score assigned by the reward model to the generated response.\n",
      "    *   **Policy:** The LLM's parameters define its policy – how it chooses to generate responses given a prompt.\n",
      "*   **RL Algorithm:** A reinforcement learning algorithm (commonly Proximal Policy Optimization - PPO) is used to update the LLM's policy (parameters) to maximize the expected reward. The goal is to train the LLM to generate responses that the reward model (and therefore, ideally, humans) will rate highly.\n",
      "*   **Policy Regularization:** A crucial aspect of this step is *policy regularization*.  Since the reward model is only an approximation of human preferences, it's important to prevent the LLM from exploiting the reward model by generating responses that are good *according to the model* but not necessarily good *according to humans*.  This is achieved by adding a penalty term to the RL objective that discourages the LLM from deviating too far from its original, pre-RL fine-tuned policy. This helps maintain the general capabilities of the LLM while aligning it with human preferences.  This penalty is often based on the Kullback-Leibler (KL) divergence between the current policy and the initial (pre-RLHF) policy.\n",
      "\n",
      "**5. Iteration and Refinement:**\n",
      "\n",
      "*   **Repeat:**  The process of generating responses, gathering human feedback, training the reward model, and fine-tuning the LLM is often repeated iteratively.  With each iteration, the LLM becomes better aligned with human preferences.  The reward model also becomes more accurate.\n",
      "*   **Refinement of Prompts & Feedback:**  Over time, the prompts used for evaluation and the feedback provided by human annotators can also be refined to target specific areas where the LLM needs improvement.\n",
      "\n",
      "**In summary, RLHF involves:**\n",
      "\n",
      "1.  **Gathering Human Preferences:**  Collecting data on which responses humans prefer in various situations.\n",
      "2.  **Learning a Reward Model:**  Training a model to predict human preferences for different responses.\n",
      "3.  **Reinforcement Learning Fine-tuning:**  Using the reward model to guide the LLM toward generating outputs that align with human preferences, while also maintaining its general language capabilities.\n",
      "\n",
      "**Why is RLHF important?**\n",
      "\n",
      "*   **Alignment:**  It helps to align LLMs with human values, making them more helpful, honest, and harmless.\n",
      "*   **Preference Learning:** It allows LLMs to learn complex and nuanced human preferences that are difficult to express explicitly through rules or constraints.\n",
      "*   **Control:**  It provides a way to control the behavior of LLMs and guide them towards specific desired outputs.\n",
      "*   **Improved Generalization:** RLHF can lead to better generalization performance on unseen tasks and prompts.\n",
      "*   **Safer and More Responsible AI:** By aligning LLMs with human values, RLHF contributes to the development of safer and more responsible AI systems.\n",
      "\n",
      "**Challenges of RLHF:**\n",
      "\n",
      "*   **Cost and Scalability:** Gathering human feedback is expensive and time-consuming.  Scaling RLHF to very large models and datasets is a significant challenge.\n",
      "*   **Bias in Human Feedback:** Human preferences can be subjective and biased.  It's important to mitigate the impact of these biases during the training process.  Careful selection and training of annotators is crucial.\n",
      "*   **Reward Hacking:** The LLM can sometimes find ways to exploit the reward model and generate responses that are good according to the model but not actually aligned with human preferences.  This requires careful monitoring and refinement of the reward model.\n",
      "*   **Stability and Robustness:**  RL fine-tuning can sometimes be unstable and lead to degradation in performance on some tasks.  Robust training techniques are needed to ensure that the LLM remains stable and generalizable.\n",
      "*   **Defining Evaluation Metrics:** Quantifying the success of RLHF is difficult. Developing metrics that accurately capture the alignment of LLMs with human values is an ongoing challenge.\n",
      "\n",
      "RLHF is a complex but powerful technique that is playing an increasingly important role in the development of advanced language models. It's an active area of research and development, with new techniques and improvements being developed all the time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915b145-1ba6-4a95-a2d7-f78a510f08c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
